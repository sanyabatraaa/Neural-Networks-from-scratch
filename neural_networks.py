# -*- coding: utf-8 -*-
"""Neural Networks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e_GpezjOmBGpInzg6e5TDd5ZjmJe6wRE
"""

inputs= [1,2,3]
weights=[0.2,0.8,-0.5]
bias=2.0
outputs= (inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+bias)
print(outputs)

inputs=[1,2,3,2.5]
weights=[[0.2,0.8,-0.5,1],
[0.5,-0.91,0.26,-0.5],[-0.26,-0.27,0.17,0.87]]
weights1 = weights[0] #LIST OF WEIGHTS ASSOCIATED WITH 1ST NEURON : W11, W12, W13, W14
weights2 = weights[1] #LIST OF WEIGHTS ASSOCIATED WITH 2ND NEURON : W21, W22, W23, W24
weights3 = weights[2] #LIST OF WEIGHTS ASSOCIATED WITH 3RD NEURON : W31, W32, W33, W34
biases=[2,3,0.5]
bias1=2
bias2=3
bias3=0.5

outputs=[inputs[0]*weights1[0]+inputs[1]*weights1[1]+inputs[2]*weights1[2]+bias1,
        inputs[0]*weights2[0]+inputs[1]*weights2[1]+inputs[2]*weights2[2]+bias2,
        inputs[0]*weights3[0]+inputs[1]*weights3[1]+inputs[2]*weights3[2]+bias3
        ]
print(outputs)

inputs=[1,2,3,2.5]
weights=[[0.2,0.8,-0.5,1],[0.5,-0.91,0.26,-0.5],[-0.26,-0.27,0.17,0.87]]
biases=[2,3,0.5]
layer_outputs=[]
for neuron_weights,neuron_bias in zip(weights,biases):
  neuron_output=0
  for n_input,weight in zip(inputs,neuron_weights):
    neuron_output+=n_input*weight
  neuron_output+=bias
  layer_outputs.append(neuron_output)
print(layer_outputs)

#single neuron using numpy
import numpy as np
inputs=[1.0,2.0,3.0,2.5]
weights=[0.2,0.8,-0.5,1.0]
bias=2.0
inputs_array=np.array(inputs)
weights_array= np.array(weights)
outputs= np.dot(inputs_array,weights_array)+bias
print(outputs)

# layer of neuron using numpy
inputs=[1.0,2.0,3.0,2.5]
weights=[[0.2,0.8,-0.5,1],[0.5,-0.91,0.26,-0.5],[-0.26,-0.27,0.17,0.87]]
biases=[2.0,3.0,0.5]
inputs_array= np.array(inputs)
weights_array=np.array(weights)
biases_array= np.array(biases)
layer_outputs= np.dot(weights_array,inputs_array)+biases_array
layer_outputs1= np.dot(inputs_array,weights_array.T)+biases_array
print(layer_outputs)
print(layer_outputs1)

#layer of neurons and batch of data using numpy
import numpy as np
inputs=[[1.0,2.0,3.0,2.5],[2.0,5.0,-1.0,2.0],[-1.5,2.7,3.3,-0.8]]
weights=[[0.2,0.8,-0.5,1],[0.5,-0.91,0.26,-0.5],[-0.26,-0.27,0.17,0.87]]
biases=[2.0,3.0,0.5]
inputs_array= np.array(inputs)
weights_array= np.array(weights)
biases_array= np.array(biases)
outputs= np.dot(inputs_array,weights_array.T)+biases_array
print(outputs)

# 2 layers and batch of data using numpy
import numpy as np
inputs=[[1.0,2.0,3.0,2.5],[2.0,5.0,-1.0,2.0],[-1.5,2.7,3.3,-0.8]]
weights=[[0.2,0.8,-0.5,1],[0.5,-0.91,0.26,-0.5],[-0.26,-0.27,0.17,0.87]]
biases=[2.0,3.0,0.5]
weights2=[[0.1,-0.14,0.5],[-0.5,0.12,-0.33],[-0.44, 0.73, -0.13]]
biases2=[-1,2,-0.5]
inputs_array=np.array(inputs)
weights_array= np.array(weights)
biases_array= np.array(biases)
weights2_array= np.array(weights2)
biases2_array= np.array(biases2)
layer1_outputs= np.dot(inputs_array,weights_array.T)+biases_array
layer2_outputs=np.dot(layer1_outputs,weights2_array.T)+biases2_array
print(layer2_outputs)

pip install nnfs

from nnfs.datasets import spiral_data
import numpy as np
import nnfs
nnfs.init()
import matplotlib.pyplot as plt
X,y= spiral_data(samples=100,classes=3)
plt.scatter(X[:,0],X[:,1])
plt.show()

plt.scatter(X[:,0],X[:,1],c=y,cmap='brg')
plt.show()

import numpy as np
import nnfs
from nnfs.datasets import spiral_data
nnfs.init()
class Layer_Dense:
  def __init__(self,n_inputs,n_neurons):
    self.weights= 0.01* np.random.randn(n_inputs,n_neurons)
    self.biases= np.zeros((1,n_neurons))

  def forward(self,inputs):
    self.output= np.dot(inputs,self.weights)+self.biases
X,y = spiral_data(samples=100,classes=3)
dense1= Layer_Dense(2,3)
dense1.forward(X)
print(dense1.output[:5])

#relu activation
class Activation_ReLU:
  def forward(self,inputs):
    self.output= np.maximum(0,inputs)

X,y = spiral_data(samples=100,classes=3)
dense1= Layer_Dense(2,3)
activation1= Activation_ReLU()
dense1.forward(X)
activation1.forward(dense1.output)
print(activation1.output[:5])

A= [[1,2,3],[4,5,6],[7,8,9]]
print(np.sum(A))
print(np.sum(A,axis=0))
print(np.sum(A,axis=0).shape)
print(np.sum(A,axis=1))
print(np.sum(A,axis=1).shape)
print(np.sum(A,axis=0,keepdims=True))
print(np.sum(A,axis=0,keepdims=True).shape)
print(np.sum(A,axis=1,keepdims=True))
print(np.sum(A,axis=1,keepdims=True).shape)
print(np.max(A,axis=0))
print(np.max(A,axis=1))

#softmax activation
class Activation_Softmax:
  def forward(self,inputs):
    exp_values= np.exp(inputs-np.max(inputs,axis=1,keepdims=True))
    probabilities= exp_values/np.sum(exp_values,axis=1,keepdims=True)
    self.output= probabilities

X,y = spiral_data(samples=100,classes=3)
dense1= Layer_Dense(2,3)
activation1= Activation_ReLU()
dense2= Layer_Dense(3,3)
activation2= Activation_Softmax()
dense1.forward(X)
activation1.forward(dense1.output)
dense2.forward(activation1.output)
activation2.forward(dense2.output)
print(activation2.output[:5])

softmax_outputs= np.array([[0.7,0.1,0.2],[0.1,0.5,0.4],[0.02,0.9,0.08]])
class_targets= [0,1,1]
print(softmax_outputs[[0,1,2],[class_targets]])

neg_log= -np.log(softmax_outputs[range(len(softmax_outputs)),class_targets])
avg_loss= np.mean(neg_log)
print(avg_loss)

y_true_check= np.array([[0,1,0],[1,0,0],[0,0,1]])
y_pred_clipped_check= np.array([[0.2,0.7,0.1],[0.8,0.1,0.1],[0.1,0.2,0.7]])
y_true_check*y_pred_clipped_check

class Loss:
  def calculate(self,output,y):
    sample_losses= self.forward(output,y)
    data_loss= np.mean(sample_losses)
    return data_loss

class Loss_CategoricalCrossentropy(Loss):
  def forward(self,y_pred,y_true):
    samples= len(y_pred)
    y_pred_clipped= np.clip(y_pred,1e-7,1-1e-7)
    if len(y_true.shape)==1:
      correct_confidences= y_pred_clipped[range(samples),y_true]
    elif len(y_true.shape)==2:
      correct_confidences= np.sum(y_pred_clipped*y_true,axis=1)
    negative_log_likelihoods= -np.log(correct_confidences)
    return negative_log_likelihoods

  def backward(self,dvalues,y_true):
    samples= len(dvalues)
    labels= len(dvalues[0])
    if len(y_true.shape)==1:
      y_true= np.eye(labels)[y_true]
    self.dinputs= -y_true/dvalues
    self.dinputs= self.dinputs/samples

softmax_outputs= np.array([[0.7,0.1,0.2],[0.1,0.5,0.4],[0.02,0.9,0.08]])
class_targets= np.array([[1,0,0],[0,1,0],[0,1,0]])
loss_function= Loss_CategoricalCrossentropy()
loss= loss_function.calculate(softmax_outputs,class_targets)
print(loss)

X,y= spiral_data(samples=100,classes=3)
dense1= Layer_Dense(2,3)
activation1= Activation_ReLU()
dense2= Layer_Dense(3,3)
activation2= Activation_Softmax()
loss_function= Loss_CategoricalCrossentropy()
dense1.forward(X)
activation1.forward(dense1.output)
dense2.forward(activation1.output)
activation2.forward(dense2.output)
print(activation2.output[:5])
loss= loss_function.calculate(activation2.output,y)
print("loss",loss)
predictions= np.argmax(activation2.output,axis=1)
print("pred",predictions)
if len(y.shape)==2:
  y=np.argmax(y,axis=1)
  print("y",y)
accuracy= np.mean(predictions==y)
print("acc:",accuracy)

import numpy as np
softmax_outputs= np.array([[0.7,0.2,0.1],[0.5,0.1,0.4],[0.02,0.9,0.08]])
class_targets= np.array([0,1,1])
predictions= np.argmax(softmax_outputs,axis=1)
if len(class_targets.shape)==2:
  class_targets= np.argmax(class_targets,axis=1)
accuracy= np.mean(predictions==class_targets)
print("Accuracy",accuracy)

#backpropagation on a single neuron
import numpy as np
weights=np.array([-3.0,-1.0,2.0])
bias=1.0
inputs= np.array([1.0,-2.0,3.0])
target_output= 0.0
learning_rate= 0.001
def relu(x):
  return np.maximum(0,x)
def relu_derivative(x):
  return np.where(x>0,1.0,0.0)
for iteration in range(200):
  linear_output= np.dot(weights,inputs)+bias
  output= relu(linear_output)
  loss= (output-target_output)**2

  #backward pass
  dloss_doutput= 2*(output-target_output)
  doutput_dlinear= relu_derivative(linear_output)
  dlinear_dweights= inputs
  dlinear_dbias=1.0

  dloss_dlinear= dloss_doutput*doutput_dlinear
  dloss_dweights= dloss_dlinear*dlinear_dweights
  dloss_dbias= dloss_dweights*dlinear_dbias

  weights-=learning_rate*dloss_dweights
  bias-=learning_rate*dloss_dbias
  print(f"Iteration {iteration+1}, Loss: {loss}")

print("Final weights:",weights)
print("Final bias:",bias)

#backpropagation layer
import numpy as np
inputs= np.array([1,2,3,4])
weights= np.array([[0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8],[0.9,1.0,1.1,1.2]])
biases= np.array([0.1,0.2,0.3])
learning_rate= 0.001
def relu(x):
  return np.maximum(0,x)

def relu_derivative(x):
  return np.where(x>0,1,0)

for iteration in range(200):
  z= np.dot(weights,inputs)+biases
  a= relu(z)
  y= np.sum(a)
  loss= y**2

  dL_dy= 2*y
  dy_da= np.ones_like(a)
  dL_da= dL_dy*dy_da
  da_dz= relu_derivative(z)
  dL_dz= dL_da*da_dz
  dL_dW=np.outer(dL_dz,inputs)
  dL_db= dL_dz

  weights-=learning_rate*dL_dW
  biases-=learning_rate*dL_db

  if iteration%20==0:
    print(f"Iteration {iteration}, Loss: {loss}")
print("Final weights",weights)
print("Final biases",biases)

import numpy as np
dvalues= np.array([[1.,1.,1.],[2.,2.,2.],[3.,3.,3.]])
inputs= np.array([[1,2,3,2.5],[2.,5.,-1.,2],[-1.5,2.7,3.3,-0.8]])
dweights= np.dot(inputs.T,dvalues)
print(dweights)

import numpy as np
dvalues= np.array([[1.,1.,1.],[2.,2.,2.],[3.,3.,3.]])
biases= np.array([[2,3,0.5]])
dbiases= np.sum(dvalues,axis=0,keepdims=True)
print(dbiases)

import numpy as np
dvalues= np.array([[1,1,1],[2,2,2],[3,3,3]])
weights=np.array([[0.2,0.8,-0.5,1],[0.5,-0.91,0.26,-0.5],[-0.26,-0.27,0.17,0.87]]).T
dinputs= np.dot(dvalues,weights.T)
print(dinputs)

# combined softmax and categorical cross entropy (#3)
class Activation_Softmax_Loss_CategoricalCrossentropy:
  def __init__(self):
    self.activation= Activation_Softmax()
    self.loss= Loss_CategoricalCrossentropy()

  def forward(self,inputs,y_true):
    self.activation.forward(inputs)
    self.output= self.activation.output
    return self.loss.calculate(self.output,y_true)

  def backward(self,dvalues,y_true):
    samples= len(dvalues)
    if len(y_true.shape)==2:
      y_true= np.argmax(y_true,axis=1)
    self.dinputs=dvalues.copy()
    self.dinputs[range(samples),y_true]-=1
    self.dinputs= self.dinputs/samples

softmax_outputs= np.array([[0.7,0.1,0.2],[0.1,0.5,0.4],[0.02,0.9,0.08]])
class_targets= np.array([0,1,1])
softmax_loss= Activation_Softmax_Loss_CategoricalCrossentropy()
softmax_loss.backward(softmax_outputs,class_targets)
dvalues1 = softmax_loss.dinputs
print("Gradient combined loss and activation")
print(dvalues1)

#2
class Activation_ReLU:
  def forward(self,inputs):
    self.inputs= inputs
    self.output= np.maximum(0,inputs)

  def backward(self,dvalues):
    self.dinputs= dvalues.copy()
    self.dinputs[self.inputs<=0]=0

#1
class Layer_Dense:
  def __init__(self,n_inputs,n_neurons):
    self.weights= 0.01*np.random.randn(n_inputs,n_neurons)
    self.biases= np.zeros((1,n_neurons))

  def forward(self,inputs):
    self.inputs= inputs
    self.output= np.dot(inputs,self.weights)+self.biases

  def backward(self,dvalues):
    self.dweights= np.dot(self.inputs.T,dvalues)
    self.dbiases= np.sum(dvalues,axis=0,keepdims=True)
    self.dinputs= np.dot(dvalues,self.weights.T)

from nnfs.datasets import spiral_data
import numpy as np
import nnfs
nnfs.init()
import matplotlib.pyplot as plt
X, y = spiral_data(samples=100, classes=3)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='brg')
plt.show()

dense1= Layer_Dense(2,3)
activation1= Activation_ReLU()
dense2= Layer_Dense(3,3)
loss_Activation= Activation_Softmax_Loss_CategoricalCrossentropy()
dense1.forward(X)
activation1.forward(dense1.output)
dense2.forward(activation1.output)
loss= loss_Activation.forward(dense2.output,y)
print(loss_Activation.output[:5])
print("loss:",loss)
predictions= np.argmax(loss_Activation.output,axis=1)
if len(y.shape)==2:
  y= np.argmax(y,axis=1)
accuracy= np.mean(y==predictions)
print("Accuracy:",accuracy)
loss_Activation.backward(loss_Activation.output,y)
dense2.backward(loss_Activation.dinputs)
activation1.backward(dense2.dinputs)
print(dense1.weights)
print(dense1.biases)
print(dense2.weights)
print(dense2.biases)

# learning rate decay
class Optimizer_SGD:
  def __init__(self,learning_rate=1.,decay=0.):
    self.learning_rate= learning_rate
    self.current_learning_rate= learning_rate
    self.decay= decay
    self.iterations=0

  def pre_update_params(self):
    if self.decay:
      self.current_learning_rate= self.learning_rate * \
      (1./(1.+self.decay*self.iterations))

  def update_params(self,layer):
    layer.weights+=-self.current_learning_rate*layer.weights
    layer.biases+=-self.current_learning_rate*layer.biases

  def post_update_params(self):
    self.iterations+=1

import numpy as np

# Assuming the necessary classes (Layer_Dense, Activation_ReLU,
# Activation_Softmax_Loss_CategoricalCrossentropy, and spiral_data) are defined elsewhere

X, y = spiral_data(samples=100, classes=3)

# Create Dense layer with 2 input features and 64 output values
dense1 = Layer_Dense(2, 64)

# Create ReLU activation (to be used with Dense layer)
activation1 = Activation_ReLU()

# Create second Dense layer with 64 input features (as we take output
# of previous layer here) and 3 output values (output values)
dense2 = Layer_Dense(64, 3)

# Create Softmax classifier's combined loss and activation
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# Create optimizer
optimizer = Optimizer_SGD(decay=1e-3)

# Train in loop
for epoch in range(10001):
    # Perform a forward pass of our training data through this layer
    dense1.forward(X)

    # Perform a forward pass through activation function
    # takes the output of first dense layer here
    activation1.forward(dense1.output)

    # Perform a forward pass through second Dense layer
    # takes outputs of activation function of first layer as inputs
    dense2.forward(activation1.output)

    # Perform a forward pass through the activation/loss function
    # takes the output of second dense layer here and returns loss
    loss = loss_activation.forward(dense2.output, y)

    # Calculate accuracy from output of activation2 and targets
    # calculate values along first axis
    predictions = np.argmax(loss_activation.output, axis=1)
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)
    accuracy = np.mean(predictions == y)

    if not epoch % 100:
        print(f'epoch: {epoch}, ' +
              f'acc: {accuracy:.3f}, ' +
              f'loss: {loss:.3f}, ' +
              f'lr: {optimizer.current_learning_rate}')

    # Backward pass
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    # Update weights and biases
    optimizer.pre_update_params()
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params()

